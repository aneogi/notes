

-------------------------------------
Question 4: Incorrect
An application has been making use of AWS DynamoDB for its back-end data store. The size of the table has now grown to 20 GB , and the scans on the table are causing throttling errors. Which of the following should now be implemented to avoid such errors?

Explanation
When you scan your table in Amazon DynamoDB, you should follow the DynamoDB best practices for avoiding sudden bursts of read activity.You can use the following technique to minimize the impact of a scan on a table's provisioned throughput.Reduce page sizeBecause a Scan operation reads an entire page (by default, 1 MB), you can reduce the impact of the scan operation by setting a smaller page size. The Scan operation provides a Limit parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a "pause" between each request. For example, suppose that each item is 4 KB and you set the page size to 40 items. A Query request would then consume only 20 eventually consistent read operations or 40 strongly consistent read operations. A larger number of smaller Query or Scan operations would allow your other critical requests to succeed without throttling.For more information, please check below AWS Docs:https://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.htmlThe correct answer is: Reduced page size

-------------------------------------
Question 6: Incorrect
Your application currently makes use of AWS Cognito for managing user identities. You want to analyze the information that is stored in AWS Cognito for your application. Which of the following features of AWS Cognito should you use for this purpose?

Explanation
The AWS Documentation mentions the followingAmazon Cognito Streams gives developers control and insight into their data stored in Amazon Cognito. Developers can now configure a Kinesis stream to receive events as data is updated and synchronized. Amazon Cognito can push each dataset change to a Kinesis stream you own in real time.All other options are invalid since you should use Cognito StreamsFor more information on Cognito Streams, please refer to the below linkhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-streams.htmlThe correct answer is: Cognito Streams

-------------------------------------
Question 8: Incorrect
You have the following YAML file given to you which is required to deploy a Lambda function using serverless deployment.AWSTemplateFormatVersion: '2010-09-09'Transform: AWS::Serverless-2016-10-31Resources: TestFunction: Type: AWS::Serverless::Function Properties: Handler: index.handler Runtime: nodejs6.10 Environment: Variables: S3_BUCKET: demobucketWhich of the following is required to ensure the deployment can take place?

Explanation
The above snippet is used to create a serverless application that is deployed using the serverless deployment language. You need to ensure that the Lambda function is present as part of the deployment packageOptions A and B are incorrect since these are not cloudformation specific templatesOption D is incorrect since this is normally used for Elastic beanstalk deploymentsFor more information on serverless deployment , please refer to the below URLhttps://docs.aws.amazon.com/lambda/latest/dg/serverless-deploy-wt.htmlThe correct answer is: Place the function code in the root directory along with the YAML file

-------------------------------------
Question 10: Incorrect
An organization is using an Amazon ElastiCache cluster in front of their Amazon RDS instance. The organization would like the Developer to implement logic into the code so that the cluster only retrieves data from RDS when there is a cache miss.What strategy can the Developer implement to achieve this?

Explanation
The AWS Documentation mentions the different caching strategies, for the above scenario the best one to choose is Lazy Loading.Whenever your application requests data, it first makes the request to the ElastiCache cache. If the data exists in the cache and is current, ElastiCache returns the data to your application. If the data does not exist in the cache, or the data in the cache has expired, your application requests the data from your data store which returns the data to your application. Your application then writes the data received from the store to the cache so it can be more quickly retrieved next time it is requested.All other options are incorrect since there is only one which matches the requirement of the question.For more information on the strategies for ElastiCache, please refer to the below Link:https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.htmlThe correct answer is: Lazy loading

-------------------------------------
Question 15: Incorrect
As a developer you have created a Lambda function that is used to work with a bucket in Amazon S3. The Lambda function is not working as expected. You need to debug the issue and understand what’s the underlying issue. How can you accomplish this?

Explanation
This is also mentioned in the AWS DocumentationYou can insert logging statements into your code to help you validate that your code is working as expected. Lambda automatically integrates with Amazon CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function (/aws/lambda/<function name>). Option A is incorrect since the metrics will only give the rate at which the function is executing , but not help debug the actual errorOption C is incorrect since there is no such optionOption D is incorrect since this is only used for API monitoringFor more information on monitoring functions , please refer to the below URLhttps://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.htmlThe correct answer is: Put logging statements in your code

-------------------------------------
Question 19: Incorrect
You need to migrate an existing on-premise application on AWS. It is a legacy-based application with little development support. Which of the following would be the best way to host this service in AWS?

Explanation
Since the application is a legacy-based application with little development support, porting the application onto AWS Lambda would be difficult. Hence Option C and D would be incorrect in this case.Using EBS Backed Volumes is better for durability, than Instance store in which you could lose the data if the instance is stopped.For more information on Amazon EC2, please refer to the below Link:https://aws.amazon.com/ec2/The correct answer is: EC2 Instances with EBS Backed Volumes

-------------------------------------
Question 26: Incorrect
You have a number of Lambda functions that need to be deployed using AWS CodeDeploy. The lambda functions have gone through multiple code revisions and versioning in Lambda is being used to maintain the revisions. Which of the following must be done to ensure that the right version of the function is deployed in AWS CodeDeploy?

Explanation
The AWS Documentation mentions the followingIf your application uses the AWS Lambda compute platform, the AppSpec file can be formatted with either YAML or JSON. It can also be typed directly into an editor in the console. The AppSpec file is used to specify:· The AWS Lambda function version to deploy.· The functions to be used as validation tests.All other options are incorrect since the right approach is to use the AppSpec file.For more information on the application specification files, please refer to the below Link:https://docs.aws.amazon.com/codedeploy/latest/userguide/application-specification-files.htmlThe correct answer is: Specify the version to be deployed in the AppSpec file.

-------------------------------------
Question 29: Incorrect
A Developer has been asked to create an AWS Elastic Beanstalk environment for a production web application which needs to handle thousands of requests. Currently the dev environment is running on a t1 micro instance.How can the Developer change the EC2 instance type to m4.large?

Explanation
The Elastic Beanstalk console and EB CLI set configuration options when you create an environment. You can also set configuration options in saved configurations and configuration files. If the same option is set in multiple locations, the value used is determined by the order of precedence.Configuration option settings can be composed in text format and saved prior to environment creation, applied during environment creation using any supported client, and added, modified or removed after environment creation.During environment creation, configuration options are applied from multiple sources with the following precedence, from highest to lowest:Settings applied directly to the environment – Settings specified during a create environment or update environment operation on the Elastic Beanstalk API by any client, including the AWS Management Console, EB CLI, AWS CLI, and SDKs. The AWS Management Console and EB CLI also apply recommended values for some options that apply at this level unless overridden.Saved Configurations – Settings for any options that are not applied directly to the environment are loaded from a saved configuration, if specified.Configuration Files (.ebextensions) – Settings for any options that are not applied directly to the environment, and also not specified in a saved configuration, are loaded from configuration files in the .ebextensions folder at the root of the application source bundle.Configuration files are executed in alphabetical order. For example, .ebextensions/01run.config is executed before .ebextensions/02do.config.Default Values – If a configuration option has a default value, it only applies when the option is not set at any of the above levels.If the same configuration option is defined in more than one location, the setting with the highest precedence is applied. When a setting is applied from a saved configuration or settings applied directly to the environment, the setting is stored as part of the environment's configuration. These settings can be removed with the AWS CLI or with the EB CLISettings in configuration files are not applied directly to the environment and cannot be removed without modifying the configuration files and deploying a new application version. If a setting applied with one of the other methods is removed, the same setting will be loaded from configuration files in the source bundle.Option A is incorrect since the environment is already managed by the Elastic Beanstalk service and we don’t need Cloudformation for this.Option C is incorrect since the changes need to be done for the current configuration.For more information on making this change, please refer to the below Link:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.ec2.htmlThe correct answer is: Create a configuration file in Amazon S3 with the instance type as m4.large and use the same during environment creation.

-------------------------------------
Question 30: Incorrect
Your company is planning on using the Simple Storage service to host objects that will be accessed by users. There is a speculation that there would be roughly 6000 GET requests per second. Which of the following is the right way to use object keys for optimal performance?

Explanation
The AWS Documentation mentions the following on optimal performance for S3All other options are incorrect since they are not the right ways to store object keys for optimal performanceFor more information on performance improvement , please refer to the below URLhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-performance-improve/Note: Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/POST/DELETE and 5,500 GET requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket. It is simple to increase your read or write performance exponentially. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.For more details, please check below AWS Docs: https://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.htmlThe correct answer is: exampleawsbucket/232a-2019-14-03-15-00-00/photo1.jpg

-------------------------------------
Question 33: Incorrect
Your mobile application includes a photo-sharing service that is expecting tens of thousands of users at launch. You will leverage Amazon Simple Storage Service (S3) for storage of the user Images, and you must decide how to authenticate and authorize your users for access to these images. You also need to manage the storage of these images. Which two of the following approaches should you use? Choose two answers from the options below

Explanation
The AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users).The token can then be used to grant access to the objects in S3.You can then provides access to the objects based on the key values generated via the user id.Option A is possible but then becomes a maintenance overhead because of the number of buckets.Option B is invalid because IAM users is not a good security practice.Option D is invalid because SMS tokens are not efficient for this requirement.For more information on the Security Token Service please refer to the below linkhttps://docs.aws.amazon.com/STS/latest/APIReference/Welcome.htmlThe correct answers are: Authenticate your users at the application level, and use AWS Security Token Service (STS) to grant token-based authorization to S3 objects., Use a key-based naming scheme comprised from the user IDs for all user objects in a single Amazon S3 bucket.

-------------------------------------
Question 38: Incorrect
Your planning on deploying a built application onto an EC2 Instance. There will be a number of tests conducted on this Instance. You want to have the ability to capture the logs from the web server so that it can help diagnose any issues if they occur? How can you achieve this?

Explanation
You can install the Cloudwatch agent on the machine and then configure it to send the logs for the web server to a central location in Cloudwatch.Option A is invalid since this is used for API monitoring activityOption C is invalid since this is used for just getting the network traffic coming to an Instance hosted in a VPCOption D is invalid since this will not give the detailed level of logs which is required.For more information on the Cloudwatch agent, please refer to the below linkhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.htmlThe correct answer is: Install the Cloudwatch agent on the Instance

-------------------------------------
Question 39: Incorrect
Your team has been instructed to develop a completely new solution onto AWS. Currently you have a limitation on the tools available to manage the complete lifecycle of the project. Which of the following service from AWS could help you in this aspect

Explanation
The AWS Documentation mentions the followingAWS CodeStar is a cloud-based service for creating, managing, and working with software development projects on AWS. You can quickly develop, build, and deploy applications on AWS with an AWS CodeStar project. An AWS CodeStar project creates and integrates AWS services for your project development toolchain. Depending on your choice of AWS CodeStar project template, that toolchain might include source control, build, deployment, virtual servers or serverless resources, and more. AWS CodeStar also manages the permissions required for project users (called team members). By adding users as team members to an AWS CodeStar project, project owners can quickly and simply grant each team member role-appropriate access to a project and its resources.Option A is incorrect since this service is used for managing CI/CD pipelinesOption B is incorrect since this service is used for managing code buildsOption C is incorrect since this service is used for managing source code versioning repositoriesFor more information on AWS CodeStar, please refer to the below Link:https://docs.aws.amazon.com/codestar/latest/userguide/welcome.htmlThe correct answer is: AWS CodeStar

-------------------------------------
Question 41: Incorrect
You have docker containers which are going to be deployed in the AWS Elastic Container Service. You need to ensure that instances of containers cannot access each other since these different instances are going to be used by individual customers. How can you accomplish this?

Explanation
Q: How does Amazon ECS isolate containers belonging to different customers?Amazon ECS schedules containers for execution on customer-controlled Amazon EC2 instances or with AWS Fargate and builds on the same isolation controls and compliance that are available for EC2 customers. Your compute instances are located in a Virtual Private Cloud (VPC) with an IP range that you specify. You decide which instances are exposed to the Internet and which remain private.Your EC2 instances use an IAM role to access the ECS service.Your ECS tasks use an IAM role to access services and resources.Security Groups and networks ACLs allow you to control inbound and outbound network access to and from your instances.You can connect your existing IT infrastructure to resources in your VPC using industry-standard encrypted IPsec VPN connections.You can provision your EC2 resources as Dedicated Instances. Dedicated Instances are Amazon EC2 Instances that run on hardware dedicated to a single customer for additional isolation.For more information, please check below AWS Docs:https://aws.amazon.com/ecs/faqs/Option A is incorrect since the Roles need to be assigned on the task levelOptions B and C are incorrect since access keys is not the ideal security practise.For more information on Task IAM Roles , please refer to the below URLhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.htmlThe correct answer is: Configure the Security Groups of the instances to allow only required traffic.

-------------------------------------
Question 42: Incorrect
Your team is developing a solution that will make use of DynamoDB tables. Currently the application is designed to perform scan’s on the entire table. Which of the following can be done to improve the performance of the application when it interacts with the DynamoDB table? Choose 2 answers from the options given below

Explanation
The AWS Documentation mentions the followingMany applications can benefit from using parallel Scan operations rather than sequential scans. For example, an application that processes a large table of historical data can perform a parallel scan much faster than a sequential one. Multiple worker threads in a background "sweeper" process could scan a table at a low priority without affecting production traffic. In each of these examples, a parallel Scan is used in such a way that it does not starve other applications of provisioned throughput resources.If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. For faster response times, design your tables and indexes so that your applications can use Query instead of ScanOption B is incorrect since having larger tables would just make the issue worseOption C is incorrect since this would help in the issue.For more information on scans and queries , please refer to the below URLhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.htmlThe correct answers are: Consider using parallel scans, Consider using queries

-------------------------------------
Question 44: Incorrect
Your development team has created a set of AWS lambda helper functions that would be deployed in various AWS accounts. You need to automate the deployment of these Lambda functions. Which of the following can be used to automate the deployment?

Explanation
AWS Cloudformation is a service that can be used to deploy Infrastructure as code. Here you can deploy Lambda functions to various accounts by just building the necessary templates.The other services cannot be used out of the box as they are to automate the deployment of AWS Lambda functions.For more information on Cloudformation, please refer to the below Link:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.htmlThe correct answer is: AWS Cloudformation

-------------------------------------
Question 45: Incorrect
A Developer working on an AWS CodeBuild project wants to override a build command as part of a build run to test a change. The developer has access to run the builds but does not have access to edit the CodeBuild projectWhat process should the Developer use to override the build command?

Explanation
Use the AWS CLI command to specify different parameters that need to be run for the build. Since the developer has access to run the build , he can run the build changing the parameters from the command line. The same is also mentioned in the AWS DocumentationAll other option are incorrect since we need to use the AWS CLIFor more information on running the command via the CLI, please refer to the below Link:https://docs.aws.amazon.com/codebuild/latest/userguide/run-build.html#run-build-cliThe correct answer is: Run the start build AWS CLI command with buildspecOverride property set to the new buildspec.yml file.

-------------------------------------
Question 48: Incorrect
An organization’s application needs to monitor application specific events with a standard AWS service. The service should capture the number of logged in users and trigger events accordingly. During peak times, monitoring frequency will occur every 10 seconds.What should be done to meet these requirements?

Explanation
This is clearly mentioned in the AWS DocumentationWhen creating an alarm, select a period that is greater than or equal to the frequency of the metric to be monitored. For example, basic monitoring for Amazon EC2 provides metrics for your instances every 5 minutes. When setting an alarm on a basic monitoring metric, select a period of at least 300 seconds (5 minutes). Detailed monitoring for Amazon EC2 provides metrics for your instances every 1 minute. When setting an alarm on a detailed monitoring metric, select a period of at least 60 seconds (1 minute).If you set an alarm on a high-resolution metric, you can specify a high-resolution alarm with a period of 10 seconds or 30 seconds, or you can set a regular alarm with a period of any multiple of 60 secondsOption A is incorrect since the 
-------------------------------------
Question does not mention anything on notifications.Option B is incorrect since the standard resolution counters will not help define triggers within a 10 second intervalOption D is incorrect since Cloudtrail is used for API Activity loggingFor more information on Cloudwatch metrics , please refer to the below Link:https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.htmlThe correct answer is: Create a high resolution custom Amazon CloudWatch metric

-------------------------------------
Question 51: Incorrect
An application needs to make use of an SQS queue for working with messages. An SQS queue has been created with the default settings. The application needs 60 seconds to process each message. Which of the following step need to be carried out by the application.

Explanation
If the SQS queue is created with the default settings , then the default visibility timeout is 30 seconds. And since the application needs more time for processing , you first need to change the timeout and delete the message after it is processed.Option B is incorrect since you need to process the message firstOptions C and D are incorrect since you need to change the visibility timeout for each message firstFor more information on SQS visibility timeout , please refer to the below URLhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.htmlThe correct answer is: Change the VisibilityTimeout for each message and then delete the message after processing is completed.

-------------------------------------
Question 66: Incorrect
A DynamoDB table is set with a Read Throughput capacity of 5 RCU. Which of the following read configuration will provide us the maximum number of read operations/sec?

Explanation
The calculation of throughput capacity for option B would beRead capacity(5) * Amount of data(4) = 20.Since its required at eventual consistency , we can double the read throughput to 20*2=40For Option ARead capacity(5) * Amount of data(4) = 20. Since we need strong consistency we have would get a read throughput of 20For Option CRead capacity(15) * Amount of data(1) = 15. Since we need strong consistency we have would get a read throughput of 15For Option DRead capacity(5) * Amount of data(1) = 5. Since we need eventual consistency we have would get a read throughput of 5*2=10For more information on DynamoDB throughput , please refer to the below URLhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.htmlNote: One read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of read capacity units required depends on the item size, and whether you want an eventually consistent or strongly consistent read.For example, with 5 read capacity units your application could:Perform strongly consistent reads of up to 20 KB per second (4 KB × 5 read capacity units).Perform eventually consistent reads of up to 40 KB per second (twice as much read throughput).So with eventual consistency you can read up to 40KB/sec, hence the solution is B.The correct answer is: Read capacity set to 5 for 4KB reads of data at eventual consistency

-------------------------------------
Question 67: Incorrect
A company has a cloudformation template that is used to create a huge list of resources. It creates a VPC, subnets , EC2 Instances , Autoscaling Groups , Load Balancers etc. Which of the following should be considered when designing such Cloudformation templates?

Explanation
This recommendation is also given in the AWS DocumentationAs your infrastructure grows, common patterns can emerge in which you declare the same components in each of your templates. You can separate out these common components and create dedicated templates for them. That way, you can mix and match different templates but use nested stacks to create a single, unified stack. Nested stacks are stacks that create other stacks. To create nested stacks, use the AWS::CloudFormation::Stack resource in your template to reference other templates.Option A is incorrect since this is not the recommended design practise.Options C and D are incorrect because these are used for packaging and deployment and not for the design stagesFor more information on best practises for Cloudformation , please refer to the below URLhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.htmlThe correct answer is: Look towards breaking the templates into smaller manageable templates

-------------------------------------
Question 80: Incorrect
A developer is developing an application which will make use of AWS services. You need to develop your application in such a way to compensate for any network delays. Which of the following two mechanisms would you implement in the application?

Explanation
Options A and D are incorrect since these practices would not help in the requirement for the applicationThe AWS Documentation mentions the followingIn addition to simple retries, each AWS SDK implements exponential backoff algorithm for better flow control. The idea behind exponential backoff is to use progressively longer waits between retries for consecutive error responses. You should implement a maximum delay interval, as well as a maximum number of retries. The maximum delay interval and maximum number of retries are not necessarily fixed values, and should be set based on the operation being performed, as well as other local factors, such as network latency.For more information on API retries, please refer to the below Link:https://docs.aws.amazon.com/general/latest/gr/api-retries.htmlThe correct answers are: Exponential backoff algorithm, Retries in your application code

-------------------------------------
Question 83: Incorrect
Your architect has drawn out the details for a mobile based application. Below are the key requirements when it comes to authentication· Users should have the ability to sign-in using external identities such as Facebook or Google.· There should be a facility to manage user profilesWhich of the following would you consider as part of the development process for the application?

Explanation
The AWS Documentation mentions the followingUser pools provide:Sign-up and sign-in services.A built-in, customizable web UI to sign in users.Social sign-in with Facebook, Google, and Login with Amazon, as well as sign-in with SAML identity providers from your user pool.User directory management and user profiles.Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.Customized workflows and user migration through AWS Lambda triggers.Options A and C is incorrect since this would require a lot of effort to develop and maintainOption D is incorrect since this is normally used for external directories such as Active DirectoryFor more information on user identity pools, please refer to the below Link:https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.htmlThe correct answer is: Consider using User pools in AWS Cognito

-------------------------------------
Question 88: Incorrect
An application is being developed that is going to write data to a DynamoDB table. You have to setup the read and write throughput for the table. Data is going to be read at the rate of 300 items every 30 seconds. Each item is of size 6KB. The reads can be eventual consistent reads. What should be the read capacity that needs to be set on the table?

Explanation
Since there are 300 items read every 30 seconds , that means there are (300/30) = 10 items read every second.Since each item is 6KB in size , that means , 2 reads will be required for each item.So we have total of 2*10 = 20 reads for the number of items per secondSince eventual consistency is required , we can divide the number of reads(20) by 2 , and in the end we get the Read Capacity of 10.As per the calculation , all other options become invalidFor more information on Read and Write capacity, please refer to the below linkhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.htmlThe correct answer is: 10

-------------------------------------
Question 92: Incorrect
A developer is using Amazon API Gateway as an HTTP proxy to a backend endpoint. There are three separate environments: Development, Testing, Production and three corresponding stages in the API gateway.How should traffic be directed to different backend endpoints for each of these stages without creating a separate API for each?

Explanation
The AWS Documentation mentions the following to support thisStage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of an API. They act like environment variables and can be used in your API setup and mapping templates.Option A is incorrect since this would only allow for additions of schema’sOption C is incorrect since this is only used for Authorization and would not help to differentiate the environmentsOption D is incorrect since this would help in integrating the responses to the API gatewayFor more information on Stage variables in the API gateway, please refer to the below Link:https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.htmlThe correct answer is: Use stage variables and configure the stage variables in the HTTP integration Request of the API


-------------------------------------
Question 96: Incorrect
A developer is writing an application that will run on –premises, but must access AWS services through an AWS SDK. How can the Developer allow the SDK to access the AWS services?

Explanation
When working on development, you need to use the AWS Access keys to work with the AWS ResourcesThe AWS Documentation additionally mentions the followingYou use different types of security credentials depending on how you interact with AWS. For example, you use a user name and password to sign in to the AWS Management Console. You use access keys to make programmatic calls to AWS API operations.Option A is incorrect since we need to do this from an on-premise server you cannot use an EC2 role to work with an on-premise server.Option C is incorrect. If you want to test your application on your local machine, you're going to need to generate temporary security credentials (access key id, secret access key, and session token). You can do this by using the access keys from an IAM user to call assumeRole. The result of that call will include credentials that you can use to set the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_SESSION_TOKEN (note without the token, they keys will be invalid). The SDK/CLI should then use these credentials. This will give your app a similar experience to running in an Amazon EC2 instance that was launched using an IAM role.https://forums.aws.amazon.com/thread.jspa?messageID=604424Option D is incorrect since the access keys should be on the local machineFor more information on usage of credentials in AWS , please refer to the below link:https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.htmlThe correct answer is: Create an IAM user with correct permissions, generate an access key and store it in aws credentials

-------------------------------------
Question 98: Incorrect
Your company has asked you to maintain an application using Elastic Beanstalk. They have mentioned that when updates are made to the application , that the infrastructure maintains its full capacity. Which of the following deployment methods should you use for this requirement?

Explanation
The AWS Documentation mentions the followingIf you need to maintain full capacity during deployments, you can configure your environment to launch a new batch of instances prior to taking any instances out of service. This option is called a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.Because of what the AWS Documentation , all other options are invalidFor more information on rolling version deployment in Elastic beanstalk , please refer to the below URLhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.htmlNote:The 
-------------------------------------
Question says, "Which methods will deploy code ONLY to new instances?".So, Immutable - "If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched."To perform an immutable environment update, Elastic Beanstalk creates a second, temporary Auto Scaling group behind your environment's load balancer to contain the new instances. First, Elastic Beanstalk launches a single instance with the new configuration in the new group. This instance serves traffic alongside all of the instances in the original Auto Scaling group that is running the previous configuration.When the first instance passes health checks, Elastic Beanstalk launches additional instances with the new configuration, matching the number of instances running in the original Auto Scaling group. If you need to maintain full capacity during deployments, you can configure your environment to launch a new batch of instances prior to taking any instances out of service. This option is called a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.With immutable environment updates, the autoscaling group starts with adding a single instance initially but whereas for rolling deployment with an additional batch all the new instances are launched behind the ELB and thus it maintains a full capacity during deployments.Rolling with additional batch – Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process.Please refer to the following links for more information.https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-methodhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.htmlThe correct answer is: Rolling with additional batch

-------------------------------------
Question 100: Incorrect
A Developer is building an application that needs access to an S3 bucket. An IAM role is created with the required permissions to access the S3 bucket. Which API call should the Developer use in the application so that the code can access to the S3 bucket?

Explanation
This is given in the AWS DocumentationA role specifies a set of permissions that you can use to access AWS resources. In that sense, it is similar to an IAM user. An application assumes a role to receive permissions to carry out required tasks and interact with AWS resources. The role can be in your own account or any other AWS account. For more information about roles, their benefits, and how to create and configure them, see IAM Roles, and Creating IAM Roles. To learn about the different methods that you can use to assume a role, see Using IAM Roles.ImportantThe permissions of your IAM user and any roles that you assume are not cumulative. Only one set of permissions is active at a time. When you assume a role, you temporarily give up your previous user or role permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored.To assume a role, an application calls the AWS STS AssumeRole API operation and passes the ARN of the role to use. When you call AssumeRole, you can optionally pass a JSON policy. This allows you to restrict permissions for that for the role's temporary credentials. This is useful when you need to give the temporary credentials to someone else. They can use the role's temporary credentials in subsequent AWS API calls to access resources in the account that owns the role. You cannot use the passed policy to grant permissions that are in excess of those allowed by the permissions policy of the role that is being assumed. To learn more about how AWS determines the effective permissions of a role, see Policy Evaluation Logic.All other options are invalid since the right way for the application to use the Role is to assume the role to get access to the S3 bucket.For more information on switching roles, please refer to the below Link:https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-api.htmlThe correct answer is: STS:AssumeRole

-------------------------------------
Question 107: Incorrect
You have been instructed to use the CodePipeline service for the CI/CD automation in your company. Due to security reasons , the resources that would be part of the deployment are placed in another account. Which of the following steps need to be carried out to accomplish this deployment? Choose 2 answers from the options given below

Explanation
Option B is invalid since this would go against the security policyOption D is invalid since this is not a recommended security practice.This is mentioned in the AWS DocumentationYou might want to create a pipeline that uses resources created or managed by another AWS account. For example, you might want to use one account for your pipeline and another for your AWS CodeDeploy resources. To do so, you must create a AWS Key Management Service (AWS KMS) key to use, add the key to the pipeline, and set up account policies and roles to enable cross-account access.For more information on pipelines used to access resources in another account , please refer to the below URLhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create-cross-account.htmlThe correct answers are: Define a customer master key in KMS, Add a cross account role

-------------------------------------
Question 112: Incorrect
Your application is developed to pick up metrics from several servers and push them off to Cloudwatch. At times , the application gets client 429 errors. Which of the following can be done from the programming side to resolve such errors?

Explanation
The main reason for such errors is that throttling is occurring when many requests are sent via API calls. The best way to mitigate this is to stagger the rate at which you make the API calls.This is also given in the AWS DocumentationIn addition to simple retries, each AWS SDK implements exponential backoff algorithm for better flow control. The idea behind exponential backoff is to use progressively longer waits between retries for consecutive error responses. You should implement a maximum delay interval, as well as a maximum number of retries. The maximum delay interval and maximum number of retries are not necessarily fixed values and should be set based on the operation being performed, as well as other local factors, such as network latency.Option A is invalid , because this accounts to the same thing. It’s basically the number of requests that is the issue.Option B is invalid because anyway you have to add the timestamps when sending the requestsOption D is invalid because this would not help in the issueFor more information on API retries , please refer to the below URLhttps://docs.aws.amazon.com/general/latest/gr/api-retries.htmlThe correct answer is: Use exponential backoff in your requests



#####################################################################################################################

Attempt 2

-------------------------------------
Question 8: Incorrect
A developer is writing an application that will run on –premises, but must access AWS services through an AWS SDK. How can the Developer allow the SDK to access the AWS services?

Explanation
When working on development, you need to use the AWS Access keys to work with the AWS ResourcesThe AWS Documentation additionally mentions the followingYou use different types of security credentials depending on how you interact with AWS. For example, you use a user name and password to sign in to the AWS Management Console. You use access keys to make programmatic calls to AWS API operations.Option A is incorrect since we need to do this from an on-premise server you cannot use an EC2 role to work with an on-premise server.Option C is incorrect. If you want to test your application on your local machine, you're going to need to generate temporary security credentials (access key id, secret access key, and session token). You can do this by using the access keys from an IAM user to call assumeRole. The result of that call will include credentials that you can use to set the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_SESSION_TOKEN (note without the token, they keys will be invalid). The SDK/CLI should then use these credentials. This will give your app a similar experience to running in an Amazon EC2 instance that was launched using an IAM role.https://forums.aws.amazon.com/thread.jspa?messageID=604424Option D is incorrect since the access keys should be on the local machineFor more information on usage of credentials in AWS , please refer to the below link:https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.htmlThe correct answer is: Create an IAM user with correct permissions, generate an access key and store it in aws credentials

-------------------------------------
Question 11: Incorrect
You are in charge of deploying an application that will be hosted on an EC2 Instance and sit behind an Elastic Load balancer. You have been requested to monitor the incoming connections to the Elastic Load Balancer. Which of the below options can suffice this requirement?

Explanation
The AWS Documentation mentions the followingElastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.Option A is invalid since the Cloudtrail service is used for API activity monitoringOption C is invalid since the Logs agents are installed on EC2 Instances and not on the ELBOption D is invalid since the metrics will not provide the detailed information on the incoming connectionsFor more information on Application Load balancer Logs, please refer to the below linkhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.htmlThe correct answer is: Enable access logs on the load balancer

-------------------------------------
Question 32: Incorrect
You have recently developed an AWS Lambda function to be used as a backend technology for an API gateway instance. You need to give the API gateway URL to a set of users for testing. What must be done before the users can test the API?

Explanation
This is also mentioned in the AWS DocumentationIn API Gateway, a deployment is represented by a Deployment resource. It is like an executable of an API represented by a RestApi resource. For the client to call your API, you must create a deployment and associate a stage to it. A stage is represented by a Stage resource and represents a snapshot of the API, including methods, integrations, models, mapping templates, Lambda authorizers (formerly known as custom authorizers), etcOption B is incorrect since this is only required for cross domain requests.Option C is incorrect since this is only required when you want to use your code to call the API gateway and there is no mention of that requirement in the questionOption D is incorrect since this is only required is the request is not a text-based request and there is no mention of the type of payload in the questionFor more information on setting up deployments , please refer to the below URLhttps://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-deployments.htmlThe correct answer is: Ensure that a deployment is created in the API gateway

-------------------------------------
Question 43: Incorrect
You have currently defined the following set of stages in CodePipeline?What happens if there is a failure which is detected at the Build Stage?

Explanation
The AWS Documentation mentions the followingIn AWS CodePipeline, an action is a task performed on an artifact in a stage. If an action or a set of parallel actions is not completed successfully, the pipeline stops running.Options A,B and C are incorrect since the default action will be that the entire pipeline will be stopped if the build does not succeed.For more information on Actions retry, please refer to the below Link:https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-retry.htmlThe correct answer is: The entire process will stop

-------------------------------------
Question 62: Incorrect
Your current log analysis application takes more than four hours to generate a report of the top 10 users of your web application. You have been asked to implement a system that can report this information in real time, ensure that the report is always up to date, and handle increases in the number of requests to your web application. Choose the option that is cost-effective and can fulfill the requirements.

Explanation
When you see Amazon Kinesis as an option, this becomes the ideal option to process data in real time. Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as application logs, website clickstreams, IoT telemetry data, and more into your databases, data lakes and data warehouses, or build your own real-time applications using this data. For more information on AWS Kinesis, please refer to the below linkhttps://aws.amazon.com/kinesis/The correct answer is: Post your log data to an Amazon Kinesis data stream, and subscribe your log-processing application so that is configured to process your logging data.

-------------------------------------
Question 64: Incorrect
A Developer has been asked to create an AWS Elastic Beanstalk environment for a production web application which needs to handle thousands of requests. Currently the dev environment is running on a t1 micro instance.How can the Developer change the EC2 instance type to m4.large?

Explanation
The Elastic Beanstalk console and EB CLI set configuration options when you create an environment. You can also set configuration options in saved configurations and configuration files. If the same option is set in multiple locations, the value used is determined by the order of precedence.Configuration option settings can be composed in text format and saved prior to environment creation, applied during environment creation using any supported client, and added, modified or removed after environment creation.During environment creation, configuration options are applied from multiple sources with the following precedence, from highest to lowest:Settings applied directly to the environment – Settings specified during a create environment or update environment operation on the Elastic Beanstalk API by any client, including the AWS Management Console, EB CLI, AWS CLI, and SDKs. The AWS Management Console and EB CLI also apply recommended values for some options that apply at this level unless overridden.Saved Configurations – Settings for any options that are not applied directly to the environment are loaded from a saved configuration, if specified.Configuration Files (.ebextensions) – Settings for any options that are not applied directly to the environment, and also not specified in a saved configuration, are loaded from configuration files in the .ebextensions folder at the root of the application source bundle.Configuration files are executed in alphabetical order. For example, .ebextensions/01run.config is executed before .ebextensions/02do.config.Default Values – If a configuration option has a default value, it only applies when the option is not set at any of the above levels.If the same configuration option is defined in more than one location, the setting with the highest precedence is applied. When a setting is applied from a saved configuration or settings applied directly to the environment, the setting is stored as part of the environment's configuration. These settings can be removed with the AWS CLI or with the EB CLISettings in configuration files are not applied directly to the environment and cannot be removed without modifying the configuration files and deploying a new application version. If a setting applied with one of the other methods is removed, the same setting will be loaded from configuration files in the source bundle.Option A is incorrect since the environment is already managed by the Elastic Beanstalk service and we don’t need Cloudformation for this.Option C is incorrect since the changes need to be done for the current configuration.For more information on making this change, please refer to the below Link:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.ec2.htmlThe correct answer is: Create a configuration file in Amazon S3 with the instance type as m4.large and use the same during environment creation.

-------------------------------------
Question 66: Incorrect
A company is planning on using AWS CodePipeline for their underlying CI/CD process. The code will be picked up from an S3 bucket. The company policy mandates that all data should be encrypted at rest. Which of the following measures would you take to ensure that the CI/CD process conforms to this policy? Choose 2 possible actions from the options given below.

Explanation
This is also mentioned in the AWS DocumentationThere are two ways to configure server-side encryption for Amazon S3 artifacts:· AWS CodePipeline creates an Amazon S3 artifact bucket and default AWS-managed SSE-KMS encryption keys when you create a pipeline using the Create Pipeline wizard. The master key is encrypted along with object data and managed by AWS.· You can create and manage your own customer-managed SSE-KMS keys.Options B and C are incorrect since this needs to be configured at the S3 bucket level.For more information on Encryption in S3 with CodePipeline, please refer to the below Link:https://docs.aws.amazon.com/codepipeline/latest/userguide/S3-artifact-encryption.htmlThe correct answers are: Ensure that server-side encryption is enabled on the S3 Bucket, Configure AWS KMS with customer managed keys and use it for S3 bucket encryption

-------------------------------------
Question 71: Incorrect
A company is writing a Lambda function that will run in multiple stages, such a dev, test and production. The function is dependent upon several external services, and it must call different endpoints for these services based on function’s deployment stage.What Lambda feature will enable the developer to ensure that the code references the correct endpoints when running in each stage?

Explanation
You can create different environment variables in the Lambda function that can be used to point to the different services. The below screenshot from the AWS Documentation shows how this can be done with databases.Option A is invalid since this can only be used to add metadata for the functionOption B is invalid since this is used for managing the concurrency of executionOption C is invalid since this is used for managing the different versions of your Lambda functionFor more information on AWS Lambda environment variables, please refer to the below Link:https://docs.aws.amazon.com/lambda/latest/dg/env_variables.htmlThe correct answer is: Environment variables

-------------------------------------
Question 81: Incorrect
Your development team has created a set of AWS lambda helper functions that would be deployed in various AWS accounts. You need to automate the deployment of these Lambda functions. Which of the following can be used to automate the deployment?

Explanation
AWS Cloudformation is a service that can be used to deploy Infrastructure as code. Here you can deploy Lambda functions to various accounts by just building the necessary templates.The other services cannot be used out of the box as they are to automate the deployment of AWS Lambda functions.For more information on Cloudformation, please refer to the below Link:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.htmlThe correct answer is: AWS Cloudformation

-------------------------------------
Question 106: Incorrect
A company currently allows access to their API’s to customers via the API gateway. Currently all clients have a 6-month period to move from using the older API’s to newer versions of the API’s. The code for the API is hosted in AWS Lambda. Which of the following is the ideal strategy to employ in such a situation?

Explanation
The best way is to create a separate stage in the API gateway as maybe ‘v2’ and then customers could use both API versions. They can still slowly change their usage onto the new version in this duration.Below is the concept of the API stage in the AWS DocumentationAPI stageA logical reference to a lifecycle state of your API (for example, 'dev', 'prod', 'beta', 'v2'). API stages are identified by API ID and stage name.Options A and B are incorrect since access needs to be provided via the gatewayOption D is incorrect since you need to keep both versions running side by sideFor more information on the API gateway , please refer to the below URLhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.htmlThe correct answer is: Create another stage in the API gateway